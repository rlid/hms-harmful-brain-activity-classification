{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"torch\" # you can also use tensorflow or torch\n","\n","import keras_cv\n","import keras\n","from keras import ops\n","import tensorflow as tf\n","\n","import cv2\n","import pandas as pd\n","import numpy as np\n","from glob import glob\n","from tqdm.notebook import tqdm\n","import joblib\n","\n","import matplotlib.pyplot as plt\n","import gc\n","\n","print(\"TensorFlow:\", tf.__version__)\n","print(\"Keras:\", keras.__version__)\n","print(\"KerasCV:\", keras_cv.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["REMOVE_DUPLICATE_EEG_IDS = True # if True, each row in training corresponds to a unique eeg_id\n","MAX_ROWS = None # use only MAX_ROWS rows of train.csv, set to None to use all rows\n","USE_CACHE = True # stores all accessed eeg and spectrogram files in memory\n","DATA_DIR = 'data' # /kaggle/input/hms-harmful-brain-activity-classification/\n","\n","EEG_N_WINDOWS_ONE_SIDE = 2 # number of eeg 2s windows either side of the centre window to include for features\n","SPG_N_WINDOWS_ONE_SIDE = 4 # number of spectrogram 10s windows either side of the centre window to include for features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def read_parquet_cache(path):\n","    cache = {}\n","\n","    def read_parquet(id_):\n","        if id_ in cache:\n","            return cache[id_]\n","        \n","        df = pd.read_parquet(f'{path}/{id_}.parquet')\n","        \n","        if USE_CACHE:\n","            cache[id_] = df\n","        \n","        return df\n","\n","    return read_parquet\n","\n","read_eeg = read_parquet_cache(path=f'{DATA_DIR}/train_eegs')\n","read_eeg_test = read_parquet_cache(path=f'{DATA_DIR}/test_eegs')\n","read_spg = read_parquet_cache(path=f'{DATA_DIR}/train_spectrograms')\n","read_spg_test = read_parquet_cache(path=f'{DATA_DIR}/test_spectrograms')\n","\n","def eeg_window(row, train=True):\n","    eeg_data = read_eeg(row.eeg_id) if train else read_eeg_test(row.eeg_id)\n","    if train:\n","        eeg_offset = int(row.eeg_label_offset_seconds)\n","        eeg_data = eeg_data.iloc[(200 * eeg_offset):(200 * (eeg_offset + 50))]\n","    return eeg_data\n","\n","def spg_window(row, train=True):\n","    spg_data = read_spg(row.spectrogram_id) if train else read_spg_test(row.spectrogram_id)\n","    if train:\n","        spg_offset = int(row.spectrogram_label_offset_seconds)\n","        spg_data = spg_data.loc[(spg_data.time >= spg_offset) & (spg_data.time < spg_offset + 600)]\n","        spg_data = spg_data.drop(columns=['time'])\n","    return spg_data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["N_SECS_PER_DF = 50\n","N_SECS_PER_WINDOW = 2\n","N_ROWS_PER_SEC = 200\n","\n","N_WINDOWS = N_SECS_PER_DF / N_SECS_PER_WINDOW\n","I_CENTRE_WINDOW = N_WINDOWS / 2 - 0.5\n","I_ROW_LEFT = int(N_ROWS_PER_SEC * N_SECS_PER_WINDOW * I_CENTRE_WINDOW)\n","N_ROWS_PER_WINDOW = N_ROWS_PER_SEC * N_SECS_PER_WINDOW\n","I_ROW_RIGHT = I_ROW_LEFT + N_ROWS_PER_WINDOW\n","\n","print(I_ROW_LEFT, I_ROW_RIGHT, N_ROWS_PER_WINDOW)\n","\n","def eeg_features(eeg_df, w=EEG_N_WINDOWS_ONE_SIDE):\n","    features = []\n","    for i in range(-w, w + 1):\n","        df = eeg_df.iloc[(I_ROW_LEFT + N_ROWS_PER_WINDOW * i):(I_ROW_RIGHT + N_ROWS_PER_WINDOW * i)].mean(axis=0)\n","        df.index = [f'{label}_mean_{i}' for label in df.index]\n","        features.append(df) \n","\n","        df = eeg_df.iloc[(I_ROW_LEFT + N_ROWS_PER_WINDOW * i):(I_ROW_RIGHT + N_ROWS_PER_WINDOW * i)].std(axis=0)\n","        df.index = [f'{label}_std_{i}' for label in df.index]\n","        features.append(df) \n","    return pd.concat(features, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["N_SECS_PER_DF = 600\n","N_SECS_PER_WINDOW = 10\n","N_SECS_PER_ROW = 2\n","\n","N_WINDOWS = N_SECS_PER_DF / N_SECS_PER_WINDOW\n","I_CENTRE_WINDOW = N_WINDOWS / 2 - 0.5\n","I_ROW_LEFT = int(N_SECS_PER_WINDOW * I_CENTRE_WINDOW / N_SECS_PER_ROW)\n","N_ROWS_PER_WINDOW = int(N_SECS_PER_WINDOW / N_SECS_PER_ROW)\n","I_ROW_RIGHT = I_ROW_LEFT + N_ROWS_PER_WINDOW\n","\n","print(I_ROW_LEFT, I_ROW_RIGHT, N_ROWS_PER_WINDOW)\n","\n","def spg_features(spg_df, w=SPG_N_WINDOWS_ONE_SIDE):\n","    features = []\n","    # divide the 600s window into 10s windows from the centre, and 2 5s windows on either side, the centre one (295s to 305s) has index (295-1)/2 = 147 to 152 (excl.)\n","    for i in range(-w, w + 1):\n","        df = spg_df.iloc[(I_ROW_LEFT + N_ROWS_PER_WINDOW * i):(I_ROW_RIGHT + N_ROWS_PER_WINDOW * i)].mean(axis=0)\n","        df.index = [f'{label}_mean_{i}' for label in df.index]\n","        features.append(df) \n","\n","        df = spg_df.iloc[(I_ROW_LEFT + N_ROWS_PER_WINDOW * i):(I_ROW_RIGHT + N_ROWS_PER_WINDOW * i)].std(axis=0)\n","        df.index = [f'{label}_std_{i}' for label in df.index]\n","        features.append(df) \n","    return pd.concat(features, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_csv = pd.read_csv(f'{DATA_DIR}/train.csv')\n","train_csv.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# remove duplicate eeg_ids (keeping the median one only) if needed\n","\n","def actual_median(s):\n","    return s.iloc[(s - s.median()).abs().argsort().iloc[0]]\n","\n","df_train = train_csv\n","\n","if REMOVE_DUPLICATE_EEG_IDS:\n","    df_unique_eeg = train_csv.groupby('eeg_id')[['eeg_label_offset_seconds']].agg(actual_median)\n","    df_train = pd.merge(df_unique_eeg, train_csv, on=['eeg_id', 'eeg_label_offset_seconds'], how='left')\n","\n","df_train = df_train[:MAX_ROWS]\n","\n","df_train.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["features_eeg = df_train.apply(lambda row: eeg_features(eeg_window(row)), axis=1)\n","features_eeg.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["features_spg = df_train.apply(lambda row: spg_features(spg_window(row)), axis=1)\n","features_spg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_processed = df_train\n","\n","col_features = list(features_eeg.columns) + list(features_spg.columns)\n","col_targets = list(df_train.columns[-6:])\n","\n","y = data_processed[col_targets]\n","y = y.div(y.sum(axis=1), axis=0)\n","\n","data_processed[col_targets] = y\n","\n","data_processed = pd.concat([data_processed, features_eeg, features_spg], axis=1)\n","data_processed = data_processed.fillna(0)  # fillna(0) / dropna()\n","data_processed = data_processed.reset_index()\n","\n","data_processed['sample_weight'] = 1.0 / data_processed.groupby('eeg_id')['eeg_sub_id'].transform('count')\n","\n","data_processed.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('/kaggle/input/kaggle-kl-div')\n","\n","from kaggle_kl_div import score\n","import catboost as cat\n","from catboost import CatBoostClassifier, Pool\n","from sklearn.model_selection import GroupKFold\n","\n","class_ids = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\n","\n","model = CatBoostClassifier(task_type='GPU', loss_function='MultiClass')\n","\n","train_pool = Pool(\n","        data=data_processed.loc[:, col_features],\n","        label=data_processed.loc[:, 'expert_consensus'].map(class_ids),\n","        weight=data_processed.loc[:, 'sample_weight']\n",")\n","\n","model.fit(train_pool, verbose=100)\n","model.save_model('model_full.cat')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["solution_df = data_processed.loc[:, col_targets]\n","submission_df = pd.DataFrame(model.predict_proba(train_pool), columns=solution_df.columns)\n","\n","solution_df['id'] = np.arange(len(solution_df))\n","submission_df['id'] = np.arange(len(submission_df))\n","\n","score(solution=solution_df,\n","      submission=submission_df,\n","      row_id_column_name='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_csv = pd.read_csv(f'{DATA_DIR}/test.csv')\n","\n","features_eeg = test_csv.apply(lambda row: eeg_features(eeg_window(row, train=False)), axis=1)\n","features_spg = test_csv.apply(lambda row: spg_features(spg_window(row, train=False)), axis=1)\n","\n","preds = []\n","model = CatBoostClassifier(task_type='GPU')\n","model.load_model('model_full.cat')\n","\n","test_pool = Pool(\n","    data = pd.concat([features_eeg, features_spg], axis=1)\n",")\n","\n","pred = model.predict_proba(test_pool)\n","\n","submission_1 = pd.DataFrame({'eeg_id': test_csv.eeg_id.values})\n","submission_1[col_targets] = pred\n","submission_1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Sample DataFrame\n","df = pd.DataFrame({\n","    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n","    'Age': [25, 30, 35, 40],\n","    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n","})\n","\n","# Set 'Name' column as the index of the DataFrame\n","df.set_index('Name', inplace=True)\n","\n","print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    verbose = 1  # Verbosity\n","    seed = 42  # Random seed\n","    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n","    image_size = [400, 300]  # Input image size\n","    epochs = 13 # Training epochs\n","    batch_size = 64  # Batch size\n","    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n","    drop_remainder = True  # Drop incomplete batches\n","    num_classes = 6 # Number of classes in the dataset\n","    fold = 0 # Which fold to set as validation data\n","    class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n","    label2name = dict(enumerate(class_names))\n","    name2label = {v:k for k, v in label2name.items()}\n","\n","keras.utils.set_random_seed(CFG.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BASE_PATH = \"/kaggle/input/hms-harmful-brain-activity-classification\"\n","\n","SPEC_DIR = \"/tmp/dataset/hms-hbac\"\n","os.makedirs(SPEC_DIR+'/train_spectrograms', exist_ok=True)\n","os.makedirs(SPEC_DIR+'/test_spectrograms', exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train + Valid\n","df = pd.read_csv(f'{BASE_PATH}/train.csv')\n","df['eeg_path'] = f'{BASE_PATH}/train_eegs/'+df['eeg_id'].astype(str)+'.parquet'\n","df['spec_path'] = f'{BASE_PATH}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.parquet'\n","df['spec2_path'] = f'{SPEC_DIR}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.npy'\n","df['class_name'] = df.expert_consensus.copy()\n","df['class_label'] = df.expert_consensus.map(CFG.name2label)\n","display(df.head(2))\n","\n","# Test\n","test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n","test_df['eeg_path'] = f'{BASE_PATH}/test_eegs/'+test_df['eeg_id'].astype(str)+'.parquet'\n","test_df['spec_path'] = f'{BASE_PATH}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.parquet'\n","test_df['spec2_path'] = f'{SPEC_DIR}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.npy'\n","display(test_df.head(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a function to process a single eeg_id\n","def process_spec(spec_id, split=\"train\"):\n","    spec_path = f\"{BASE_PATH}/{split}_spectrograms/{spec_id}.parquet\"\n","    spec = pd.read_parquet(spec_path)\n","    spec = spec.fillna(0).values[:, 1:].T # fill NaN values with 0, transpose for (Time, Freq) -> (Freq, Time)\n","    spec = spec.astype(\"float32\")\n","    np.save(f\"{SPEC_DIR}/{split}_spectrograms/{spec_id}.npy\", spec)\n","\n","# Get unique spec_ids of train and valid data\n","spec_ids = df[\"spectrogram_id\"].unique()\n","\n","# Parallelize the processing using joblib for training data\n","_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n","    joblib.delayed(process_spec)(spec_id, \"train\")\n","    for spec_id in tqdm(spec_ids, total=len(spec_ids))\n",")\n","\n","# Get unique spec_ids of test data\n","test_spec_ids = test_df[\"spectrogram_id\"].unique()\n","\n","# Parallelize the processing using joblib for test data\n","_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n","    joblib.delayed(process_spec)(spec_id, \"test\")\n","    for spec_id in tqdm(test_spec_ids, total=len(test_spec_ids))\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import StratifiedGroupKFold\n","\n","sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n","\n","df[\"fold\"] = -1\n","df.reset_index(drop=True, inplace=True)\n","for fold, (train_idx, valid_idx) in enumerate(sgkf.split(df, y=df[\"class_label\"], groups=df[\"patient_id\"])):\n","    df.loc[valid_idx, \"fold\"] = fold\n","df.groupby([\"fold\", \"class_name\"])[[\"eeg_id\"]].count().T"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_augmenter(dim=CFG.image_size):\n","    augmenters = [\n","        keras_cv.layers.MixUp(alpha=2.0),\n","        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0),\n","                                     width_factor=(0.06, 0.1)), # freq-masking\n","        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1),\n","                                     width_factor=(1.0, 1.0)), # time-masking\n","    ]\n","    \n","    def augment(img, label):\n","        data = {\"images\":img, \"labels\":label}\n","        for augmenter in augmenters:\n","            if tf.random.uniform([]) < 0.5:\n","                data = augmenter(data, training=True)\n","        return data[\"images\"], data[\"labels\"]\n","    \n","    return augment\n","\n","\n","def build_decoder(with_labels=True, target_size=CFG.image_size, dtype=32):\n","    def decode_signal(path, offset=None):\n","        # Read .npy files and process the signal\n","        file_bytes = tf.io.read_file(path)\n","        sig = tf.io.decode_raw(file_bytes, tf.float32)\n","        sig = sig[1024//dtype:]  # Remove header tag\n","        sig = tf.reshape(sig, [400, -1])\n","        \n","        # Extract labeled subsample from full spectrogram using \"offset\"\n","        if offset is not None: \n","            offset = offset // 2  # Only odd values are given\n","            sig = sig[:, offset:offset+300]\n","            \n","            # Pad spectrogram to ensure the same input shape of [400, 300]\n","            pad_size = tf.math.maximum(0, 300 - tf.shape(sig)[1])\n","            sig = tf.pad(sig, [[0, 0], [0, pad_size]])\n","            sig = tf.reshape(sig, [400, 300])\n","        \n","        # Log spectrogram \n","        sig = tf.clip_by_value(sig, tf.math.exp(-4.0), tf.math.exp(8.0)) # avoid 0 in log\n","        sig = tf.math.log(sig)\n","        \n","        # Normalize spectrogram\n","        sig -= tf.math.reduce_mean(sig)\n","        sig /= tf.math.reduce_std(sig) + 1e-6\n","        \n","        # Mono channel to 3 channels to use \"ImageNet\" weights\n","        sig = tf.tile(sig[..., None], [1, 1, 3])\n","        return sig\n","    \n","    def decode_label(label):\n","        label = tf.one_hot(label, CFG.num_classes)\n","        label = tf.cast(label, tf.float32)\n","        label = tf.reshape(label, [CFG.num_classes])\n","        return label\n","    \n","    def decode_with_labels(path, offset=None, label=None):\n","        sig = decode_signal(path, offset)\n","        label = decode_label(label)\n","        return (sig, label)\n","    \n","    return decode_with_labels if with_labels else decode_signal\n","\n","\n","def build_dataset(paths, offsets=None, labels=None, batch_size=32, cache=True,\n","                  decode_fn=None, augment_fn=None,\n","                  augment=False, repeat=True, shuffle=1024, \n","                  cache_dir=\"\", drop_remainder=False):\n","    if cache_dir != \"\" and cache is True:\n","        os.makedirs(cache_dir, exist_ok=True)\n","    \n","    if decode_fn is None:\n","        decode_fn = build_decoder(labels is not None)\n","    \n","    if augment_fn is None:\n","        augment_fn = build_augmenter()\n","    \n","    AUTO = tf.data.experimental.AUTOTUNE\n","    slices = (paths, offsets) if labels is None else (paths, offsets, labels)\n","    \n","    ds = tf.data.Dataset.from_tensor_slices(slices)\n","    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n","    ds = ds.cache(cache_dir) if cache else ds\n","    ds = ds.repeat() if repeat else ds\n","    if shuffle: \n","        ds = ds.shuffle(shuffle, seed=CFG.seed)\n","        opt = tf.data.Options()\n","        opt.experimental_deterministic = False\n","        ds = ds.with_options(opt)\n","    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n","    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n","    ds = ds.prefetch(AUTO)\n","    return ds\n","\n","print(build_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_spg(npy_path, offset=None):\n","    # spg = np.load(npy_path).reshape(400, -1)\n","    file_bytes = tf.io.read_file(npy_path)\n","    spg = tf.io.decode_raw(file_bytes, tf.float32)\n","    spg = spg[32:]  # Remove header tag\n","    spg = tf.reshape(spg, [400, -1])\n","\n","    if offset is None:\n","        offset = 0\n","    spg = spg[:, offset:(offset + 300)]\n","    row_offset = offset // 2\n","    spg = spg[:, row_offset:(row_offset + 300)]\n","\n","    # spg = np.pad(spg, [[0, 0], [0, max(0, 300 - spg.shape[1])]])\n","    right_padding = tf.math.maximum(0, 300 - tf.shape(spg)[1])\n","    spg = tf.pad(spg, [[0, 0], [0, right_padding]])\n","\n","    # spg = np.clip(spg, np.exp(-30), np.exp(30))\n","    spg = tf.clip_by_value(spg, tf.math.exp(-4.), tf.math.exp(8.))\n","\n","    # spg = np.log(spg)\n","    spg = tf.math.log(spg)\n","\n","    # spg -= spg.mean()\n","    spg -= tf.math.reduce_mean(spg)\n","\n","    # spg /= spg.std()\n","    spg /= tf.math.reduce_std(spg) + 1e-6\n","\n","    # spg = np.tile(spg[..., None], (1, 1, 3))\n","    spg = tf.tile(spg[..., None], (1, 1, 3))\n","    \n","    return spg\n","\n","\n","def build_preprocess_fn(with_labels=True):\n","    def preprocess(path, offset=None):\n","        return preprocess_spg(npy_path=path, offset=offset)\n","    \n","    def preprocess_with_label(path, offset=None, label=None):\n","        spg = preprocess_spg(npy_path=path, offset=offset)\n","        label = tf.one_hot(label, CFG.num_classes)\n","        return (spg, label)\n","    \n","    return preprocess_with_label if with_labels else preprocess\n","\n","\n","def build_augment_fn(dim=CFG.image_size):\n","    augmenters = [\n","        keras_cv.layers.MixUp(alpha=2.0),\n","        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0),\n","                                     width_factor=(0.06, 0.1)), # freq-masking\n","        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1),\n","                                     width_factor=(1.0, 1.0)), # time-masking\n","    ]\n","    \n","    def augment(img, label):\n","        data = {\"images\":img, \"labels\":label}\n","        for augmenter in augmenters:\n","            if tf.random.uniform([]) < 0.5:\n","                data = augmenter(data, training=True)\n","        return data[\"images\"], data[\"labels\"]\n","    \n","    return augment\n","\n","\n","def build_dataset(paths, offsets=None, labels=None, batch_size=32,\n","                  augment=False, augment_fn=None, preprocess_fn=None, repeat=True, shuffle=1024):\n","    if augment and augment_fn is None:\n","        augment_fn = build_augment_fn()\n","\n","    if preprocess_fn is None:\n","        preprocess_fn = build_preprocess_fn(labels is not None)\n","\n","    slices = (paths, offsets) if labels is None else (paths, offsets, labels)\n","    \n","    ds = tf.data.Dataset.from_tensor_slices(slices)\n","\n","    ds = ds.map(preprocess_fn)\n","    \n","    ds = ds.repeat() if repeat else ds\n","    if shuffle: \n","        ds = ds.shuffle(shuffle, seed=CFG.seed)\n","    ds = ds.batch(batch_size)\n","    ds = ds.map(augment_fn) if augment else ds\n","    return ds\n","\n","print(build_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Sample from full data\n","sample_df = df.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\n","train_df = sample_df[sample_df.fold != CFG.fold]\n","valid_df = sample_df[sample_df.fold == CFG.fold]\n","print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n","\n","# Train\n","train_paths = train_df.spec2_path.values\n","train_offsets = train_df.spectrogram_label_offset_seconds.values.astype(int)\n","train_labels = train_df.class_label.values\n","train_ds = build_dataset(train_paths, train_offsets, train_labels, batch_size=CFG.batch_size,\n","                         repeat=True, shuffle=True, augment=True)\n","\n","# Valid\n","valid_paths = valid_df.spec2_path.values\n","valid_offsets = valid_df.spectrogram_label_offset_seconds.values.astype(int)\n","valid_labels = valid_df.class_label.values\n","valid_ds = build_dataset(valid_paths, valid_offsets, valid_labels, batch_size=CFG.batch_size,\n","                         repeat=False, shuffle=False, augment=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["imgs, tars = next(iter(train_ds))\n","\n","num_imgs = 8\n","plt.figure(figsize=(4*4, num_imgs//4*5))\n","for i in range(num_imgs):\n","    plt.subplot(num_imgs//4, 4, i + 1)\n","    img = imgs[i].numpy()[...,0]  # Adjust as per your image data format\n","    img -= img.min()\n","    img /= img.max() + 1e-4\n","    tar = CFG.label2name[np.argmax(tars[i].numpy())]\n","    plt.imshow(img)\n","    plt.title(f\"Target: {tar}\")\n","    plt.axis('off')\n","    print(img.sum())\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Build Classifier\n","model = keras_cv.models.ImageClassifier.from_preset(\n","    CFG.preset, num_classes=CFG.num_classes\n",")\n","\n","# Compile the model  \n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","              loss=keras.losses.KLDivergence())\n","\n","# Model Sumamry\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","\n","def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n","    lr_start, lr_max, lr_min = 5e-5, 6e-6 * batch_size, 1e-5\n","    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n","\n","    def lrfn(epoch):  # Learning rate update function\n","        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n","        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n","        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n","        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n","        elif mode == 'cos':\n","            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n","            phase = math.pi * decay_epoch_index / decay_total_epochs\n","            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n","        return lr\n","\n","    if plot:  # Plot lr curve if plot is True\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n","        plt.xlabel('epoch'); plt.ylabel('lr')\n","        plt.title('LR Scheduler')\n","        plt.show()\n","\n","    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback\n","\n","\n","lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ckpt_cb = keras.callbacks.ModelCheckpoint(\"best_model.keras\",\n","                                         monitor='val_loss',\n","                                         save_best_only=True,\n","                                         save_weights_only=False,\n","                                         mode='min')\n","\n","history = model.fit(\n","    train_ds, \n","    epochs=CFG.epochs,\n","    callbacks=[lr_cb, ckpt_cb], \n","    steps_per_epoch=len(train_df)//CFG.batch_size,\n","    validation_data=valid_ds, \n","    verbose=CFG.verbose\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.load_weights(\"best_model.keras\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_paths = test_df.spec2_path.values\n","test_ds = build_dataset(test_paths, batch_size=min(CFG.batch_size, len(test_df)),\n","                         repeat=False, shuffle=False, augment=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds = model.predict(test_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pred_df = test_df[[\"eeg_id\"]].copy()\n","target_cols = [x.lower()+'_vote' for x in CFG.class_names]\n","pred_df[target_cols] = preds.tolist()\n","\n","submission_2 = test_df[[\"eeg_id\"]].copy()\n","submission_2 = submission_2.merge(pred_df, on=\"eeg_id\", how=\"left\")\n","submission_2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_2 = submission_1\n","submission_1 = submission_1.set_index('eeg_id')\n","submission_2 = submission_2.set_index('eeg_id')\n","\n","sub_df = 0.5 * submission_1.add(submission_2)\n","\n","sub_df = sub_df.reset_index()\n","\n","sub_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df.to_csv(\"submission.csv\", index=False)\n","sub_df.head()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7469972,"sourceId":59093,"sourceType":"competition"},{"datasetId":4308295,"sourceId":7526248,"sourceType":"datasetVersion"},{"datasetId":4297749,"sourceId":7392733,"sourceType":"datasetVersion"},{"modelInstanceId":4598,"sourceId":6127,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
